{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325b4f3b-f726-4d07-8992-2c28442f3e14",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "**Comparing Federated Machine Learning to Centralized Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7f3a2e",
   "metadata": {},
   "source": [
    "## Imports & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "461e8e2d-cc30-45cf-bec2-c45763034af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets.utils.logging import disable_progress_bar\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.common import Metrics, Context\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.simulation import run_simulation\n",
    "from flwr_datasets import FederatedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d408b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on NVIDIA GeForce RTX 4090\n",
      "Number of GPUs: 1.0\n",
      "Number of CPU Cores: 24\n",
      "Flower 1.14.0 / PyTorch 2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    n_gpu = float(torch.cuda.device_count())\n",
    "    device_name = torch.cuda.get_device_name(DEVICE)\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    device_name = \"Apple Silicon\"\n",
    "    n_gpu = 0.0\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "    n_gpu = 0.0\n",
    "    \n",
    "print(f\"Training on {device_name}\")\n",
    "disable_progress_bar()\n",
    "n_cores = multiprocessing.cpu_count()\n",
    "print(f\"Number of GPUs: {n_gpu}\")\n",
    "print(f\"Number of CPU Cores: {n_cores}\")\n",
    "print(f\"Flower {fl.__version__} / PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b8b85d-b71e-4b9a-8de6-f89b9f36e32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5aadbdfbf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLIENTS = 200\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 8\n",
    "NUM_ROUNDS = 5\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2f82c6-0bb0-4db3-8e94-2fa98159144f",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26a7e93b-f8c6-42b3-9bf6-8a1717ba4477",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./Data/adult_train.csv')\n",
    "test_df = pd.read_csv('./Data/adult_train.csv')\n",
    "concated_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "\n",
    "label_column_name = 'income'\n",
    "x_train = train_df.drop(columns=[label_column_name]).values\n",
    "x_test = test_df.drop(columns=[label_column_name]).values\n",
    "y_train = train_df[label_column_name].values\n",
    "y_test = test_df[label_column_name].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3a95b4-e964-4d41-abdd-fe728b146c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be931bb7-d190-4ed4-9be9-4c7edb899ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(features, labels, num_clients):\n",
    "    dataset = CustomDataset(features, labels)\n",
    "    dataset_size = len(dataset)\n",
    "    partition_size = dataset_size // num_clients\n",
    "    \n",
    "    lengths = [partition_size] * num_clients\n",
    "    lengths[-1] += dataset_size % num_clients\n",
    "    partitions = random_split(dataset, lengths)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1eb5cde-ce3b-4e32-8d24-8d59ca314cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_federated_datasets(x_train, x_test, y_train, y_test, num_clients, batch_size):\n",
    "    train_partitions = partition_dataset(x_train, y_train, num_clients)\n",
    "    test_partitions = partition_dataset(x_test, y_test, num_clients)\n",
    "    \n",
    "    federated_trainloaders = []\n",
    "    federated_testloaders = []\n",
    "\n",
    "    for train_partition, test_partition in zip(train_partitions, test_partitions):\n",
    "        trainloader = DataLoader(train_partition, batch_size=batch_size, shuffle=True)\n",
    "        testloader = DataLoader(test_partition, batch_size=batch_size, shuffle=False)\n",
    "        federated_trainloaders.append(trainloader)\n",
    "        federated_testloaders.append(testloader)\n",
    "\n",
    "    return federated_trainloaders, federated_testloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804581b6-e080-4cad-ace4-3b6324eb31fc",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ced28f-6869-4b40-8fae-cd90f26d1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Binary_MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(97, 64),            \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),              \n",
    "            nn.BatchNorm1d(64),            \n",
    "            \n",
    "            nn.Linear(64, 32),             \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.BatchNorm1d(32),\n",
    "            \n",
    "            nn.Linear(32, 16),             \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(16, 1)               \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fba540a1-97ef-44fc-9a7c-1a05fbfc6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Assuming binary classification with logits output\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float()  # Ensure labels are floats for BCELoss\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate loss for the batch\n",
    "            batch_loss = criterion(outputs, labels.unsqueeze(1))  # Labels reshaped for compatibility\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "            # Convert logits to predictions\n",
    "            preds = torch.sigmoid(outputs).round()  # Threshold at 0.5 for binary classification\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=1)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    # Average loss over all batches\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d4221-d565-4d97-ae96-d7883be3e870",
   "metadata": {},
   "source": [
    "## Centralized Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d987cc1c-6619-4ada-b6be-cd0389415586",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_centralized = CustomDataset(x_train, y_train)\n",
    "test_centralized = CustomDataset(x_test, y_test)\n",
    "train_centralized_loader = DataLoader(train_centralized, batch_size=32, shuffle=True)\n",
    "test_centralized_loader = DataLoader(test_centralized, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847059fb-7733-412c-bc99-6040a6da165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_training(model, loader, criterion, optimizer, num_epochs=NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(DEVICE).float()\n",
    "            labels = labels.to(DEVICE).float().unsqueeze(1)  # Ensure correct shape\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Calculate predictions and update accuracy metrics\n",
    "            preds = torch.sigmoid(outputs).round()  # Threshold at 0.5 for binary classification\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    avg_loss = total_loss / (num_epochs * len(loader))  # Average loss over all batches\n",
    "    overall_accuracy = correct_predictions / total_samples\n",
    "    return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0d9ede4-898e-42e1-87c0-daac681fe718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8455475876048033"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Binary_MLP().to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "centralized_training(model, train_centralized_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a359bc-b1c8-4712-ba25-2b302864ac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centralized Model - Average Loss: 0.3060, Accuracy: 0.8608, Precision: 0.7657, Recall: 0.6080, F1: 0.6778\n"
     ]
    }
   ],
   "source": [
    "c_loss, c_accuracy, c_precision, c_recall, c_f1 = evaluate_model(model, test_centralized_loader, DEVICE)\n",
    "\n",
    "print(f\"Centralized Model - Average Loss: {c_loss:.4f}, Accuracy: {c_accuracy:.4f}, Precision: {c_precision:.4f}, Recall: {c_recall:.4f}, F1: {c_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f1a6755-b497-421f-9237-8d2adc8157d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight \t torch.Size([64, 97])\n",
      "model.0.bias \t torch.Size([64])\n",
      "model.3.weight \t torch.Size([64])\n",
      "model.3.bias \t torch.Size([64])\n",
      "model.3.running_mean \t torch.Size([64])\n",
      "model.3.running_var \t torch.Size([64])\n",
      "model.3.num_batches_tracked \t torch.Size([])\n",
      "model.4.weight \t torch.Size([32, 64])\n",
      "model.4.bias \t torch.Size([32])\n",
      "model.7.weight \t torch.Size([32])\n",
      "model.7.bias \t torch.Size([32])\n",
      "model.7.running_mean \t torch.Size([32])\n",
      "model.7.running_var \t torch.Size([32])\n",
      "model.7.num_batches_tracked \t torch.Size([])\n",
      "model.8.weight \t torch.Size([16, 32])\n",
      "model.8.bias \t torch.Size([16])\n",
      "model.11.weight \t torch.Size([1, 16])\n",
      "model.11.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936620d1-5d18-4c7f-b7c7-3f20ab04fe1c",
   "metadata": {},
   "source": [
    "## Federated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae044a56-866d-4ad4-8da4-f5df131f92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "backend_config = {\n",
    "    \"client_resources\": {\n",
    "        \"num_cpus\": n_cores,\n",
    "        \"num_gpus\": n_gpu\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a97a55e-0636-4cdd-9007-c8ef6cc122a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameters(model, parameters):\n",
    "    params_dict = zip(model.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.from_numpy(v) for k, v in params_dict})\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "def get_parameters(model) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ea88c69-fe7d-4421-9de1-74f5a7d1f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(NumPyClient):\n",
    "    def __init__(self, model, trainloader, testloader):\n",
    "        self.model = model\n",
    "        self.trainloader = trainloader\n",
    "        self.testloader = testloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return get_parameters(self.model)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        set_parameters(self.model, parameters)\n",
    "        accuracy = centralized_training(self.model, self.trainloader, criterion, optimizer)\n",
    "        return get_parameters(self.model), len(self.trainloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(self.model, parameters)\n",
    "        loss, accuracy, precision, recall, f1 = evaluate_model(self.model, self.testloader, DEVICE)\n",
    "        return float(loss), len(self.testloader), {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "767d35e0-6607-4e15-808b-7e189ce266a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(context: Context) -> Client:\n",
    "    model = Binary_MLP().to(DEVICE)\n",
    "    partition_id = context.node_config[\"partition-id\"]\n",
    "\n",
    "    trainloaders, testloaders = load_federated_datasets(\n",
    "        x_train,\n",
    "        x_test,\n",
    "        y_train,\n",
    "        y_test,\n",
    "        NUM_CLIENTS,\n",
    "        BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    trainloader = trainloaders[partition_id]\n",
    "    testloader = testloaders[partition_id]\n",
    "\n",
    "    return FlowerClient(model, trainloader, testloader).to_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "113c4310-6789-4ca6-9288-93b8568bf1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "242ce0d3-6fd7-42ac-9c69-b3517acb8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    strategy = FedAvg(\n",
    "        fraction_fit=1.0,  # Sample 100% of available clients for training\n",
    "        fraction_evaluate=1.0,  # Sample 100% of available clients for evaluation\n",
    "        min_fit_clients=10,  # Never sample less than 10 clients for training\n",
    "        min_evaluate_clients=5,  # Never sample less than 5 clients for evaluation\n",
    "        min_available_clients=10, # Wait until all 10 clients are available\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,\n",
    "        fit_metrics_aggregation_fn=weighted_average\n",
    "    )\n",
    "    config = ServerConfig(num_rounds=NUM_ROUNDS)\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92c43d2b-519e-4c6d-b5b3-ca6fe85a8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ClientApp(client_fn=client_fn)\n",
    "server = ServerApp(server_fn=server_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "680bc163-735c-457e-a5f9-2c1196c47437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92mINFO \u001b[0m:      Starting Flower ServerApp, config: num_rounds=5, no round_timeout\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 200 clients (out of 200)\n",
      "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 200 results and 0 failures\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
      "\u001b[92mINFO \u001b[0m:      Run finished 5 round(s) in 179.05s\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.6718805738504496\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.6735779787494929\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.6728400387209975\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 4: 0.6754233813879401\n",
      "\u001b[92mINFO \u001b[0m:      \t\tround 5: 0.6731628938829256\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, fit):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(1, 0.6094359563242444),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (2, 0.6083533403655318),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (3, 0.608963602426727),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (4, 0.605862441067276),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (5, 0.6094238117464356)]}\n",
      "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
      "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(1, 0.709256095245843),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (2, 0.683516536846958),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (3, 0.682484925736085),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (4, 0.6744002767203306),\n",
      "\u001b[92mINFO \u001b[0m:      \t              (5, 0.6861633649116144)]}\n",
      "\u001b[92mINFO \u001b[0m:      \n"
     ]
    }
   ],
   "source": [
    "run_simulation(\n",
    "    server_app=server,\n",
    "    client_app=client,\n",
    "    num_supernodes=NUM_CLIENTS,\n",
    "    backend_config=backend_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa8524c-f681-4a03-92f4-c5895a0819d8",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
